[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bansari’s website",
    "section": "",
    "text": "Welcome to my website!\nI’m a Graduate student pursuing Business Analytics :)"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Bansari’s resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "My Projects",
    "section": "",
    "text": "Segmentation Methods\n\n\n\n\n\n\nBansari Kathrotia\n\n\nJun 6, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nKey Drivers Analysis\n\n\n\n\n\n\nBansari Kathrotia\n\n\nJun 6, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\n\n\n\nBansari Kathrotia\n\n\nMay 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\n\n\n\nBansari Kathrotia\n\n\nMay 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nA Replication of Karlan and List (2007)\n\n\n\n\n\n\nBansari Kathrotia\n\n\nApr 16, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/project1/index.html",
    "href": "projects/project1/index.html",
    "title": "Homework 1",
    "section": "",
    "text": "some\nword\nin\nbullets\n\n\nsome\nnumbers\nin\na\nlist\n\ntext can be bold, or italics, or strikethrough\nMy website is https://rsm-bkathrotia.github.io/bkathrotia.github.io/ or here"
  },
  {
    "objectID": "projects/project1/index.html#sub-header",
    "href": "projects/project1/index.html#sub-header",
    "title": "My main Project",
    "section": "",
    "text": "I also analyze data."
  },
  {
    "objectID": "projects/project1/index.html#sub-section",
    "href": "projects/project1/index.html#sub-section",
    "title": "Homework 1",
    "section": "",
    "text": "some\nword\nin\nbullets\n\n\nsome\nnumbers\nin\na\nlist\n\ntext can be bold, or italics, or strikethrough\nMy website is https://rsm-bkathrotia.github.io/bkathrotia.github.io/ or here"
  },
  {
    "objectID": "projects/Assignment1/hw1_questions.html",
    "href": "projects/Assignment1/hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nto do: expand on the description of the experiment.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "projects/Assignment1/hw1_questions.html#introduction",
    "href": "projects/Assignment1/hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nto do: expand on the description of the experiment.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "projects/Assignment1/hw1_questions.html#dataset",
    "href": "projects/Assignment1/hw1_questions.html#dataset",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Dataset",
    "text": "Dataset\n\n\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio\nratio2\nratio3\nsize\nsize25\nsize50\nsize100\nsizeno\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\n0\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n1.0\n0.446493\n0.527769\n0.317591\n2.10\n28517.0\n0.499807\n0.324528\n1.000000\n\n\n1\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n1\n0\n1\n0\n0\n$100,000\n0\n0\n1\n0\n...\n0.0\n1.0\n0.935706\n0.011948\n0.276128\n2.48\n51175.0\n0.721941\n0.192668\n1.000000\n\n\n3\n1\n0\n1\n0\n0\nUnstated\n0\n0\n0\n1\n...\n1.0\n0.0\n0.888331\n0.010760\n0.279412\n2.65\n79269.0\n0.920431\n0.412142\n1.000000\n\n\n4\n1\n0\n1\n0\n0\n$50,000\n0\n1\n0\n0\n...\n0.0\n1.0\n0.759014\n0.127421\n0.442389\n1.85\n40908.0\n0.416072\n0.439965\n1.000000\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n50078\n1\n0\n1\n0\n0\n$25,000\n1\n0\n0\n0\n...\n0.0\n1.0\n0.872797\n0.089959\n0.257265\n2.13\n45047.0\n0.771316\n0.263744\n1.000000\n\n\n50079\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n1.0\n0.688262\n0.108889\n0.288792\n2.67\n74655.0\n0.741931\n0.586466\n1.000000\n\n\n50080\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.0\n0.900000\n0.021311\n0.178689\n2.36\n26667.0\n0.778689\n0.107930\n0.000000\n\n\n50081\n1\n0\n3\n0\n1\nUnstated\n0\n0\n0\n1\n...\n1.0\n0.0\n0.917206\n0.008257\n0.225619\n2.57\n39530.0\n0.733988\n0.184768\n0.634903\n\n\n50082\n1\n0\n3\n0\n1\n$25,000\n1\n0\n0\n0\n...\n0.0\n1.0\n0.530023\n0.074112\n0.340698\n3.70\n48744.0\n0.717843\n0.127941\n0.994181\n\n\n\n\n50083 rows × 51 columns\n\n\n\n\n\nDescription\nThe dataset contains 50,083 rows and 51 columns with different datatypes.\n\nKey points summarizing the dataset:\n\nExperimental Design: Tracks the impact of different fundraising treatments versus control scenarios on donor behavior.\nMatching and Donation Thresholds: Includes variables for various matching ratios and financial thresholds, influencing donation amounts.\nDonor Demographics and History: Details about donors such as gender, couple status, previous donations, and years since the first donation.\nGeographic and Socioeconomic Factors: Analyzes the influence of state and local demographics, including political alignment and socioeconomic status, on donation patterns.\n\n\n\nTreatment Percentage: 0.6668130902701516\nControl Percentage: 0.33318690972984844\n\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\n\n\nT-Test\ntodo: test a few variables other than the key outcome variables (for example, test months since last donation) to see if the treatment and control groups are statistically significantly different at the 95% confidence level. Do each as a t-test and separately as a linear regression, and confirm you get the exact same results from both methods. When doing a t-test, use the formula in the class slides. When doing the linear regression, regress for example mrm2 on treatment and look at the estimated coefficient on the treatment variable. It might be helpful to compare parts of your analysis to Table 1 in the paper. Be sure to comment on your results (hint: why is Table 1 included in the paper).\nComparing the average values of variables such as mrm2 between treatment and control groups helps us determine if there are any statistically significant differences between these groups at a 95% confidence level.\n\n# Outputting the results of the t-tests\ntreatment_vs_control_stats, control_vs_control_stats\n\n(TtestResult(statistic=0.1194921058159193, pvalue=0.9048859731777738, df=50080.0),\n TtestResult(statistic=0.0, pvalue=1.0, df=33372.0))\n\n\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                dormant   R-squared:                       0.002\nModel:                            OLS   Adj. R-squared:                  0.001\nMethod:                 Least Squares   F-statistic:                     5.652\nDate:                Mon, 29 Apr 2024   Prob (F-statistic):           1.67e-08\nTime:                        16:09:41   Log-Likelihood:                -22664.\nNo. Observations:               31307   AIC:                         4.535e+04\nDf Residuals:                   31296   BIC:                         4.544e+04\nDf Model:                          10                                         \nCovariance Type:            nonrobust                                         \n===================================================================================\n                      coef    std err          t      P&gt;|t|      [0.025      0.975]\n-----------------------------------------------------------------------------------\nIntercept           0.5521      0.053     10.411      0.000       0.448       0.656\nfemale              0.0007      0.006      0.105      0.916      -0.012       0.013\ncouple             -0.0512      0.010     -5.141      0.000      -0.071      -0.032\npwhite             -0.0293      0.041     -0.711      0.477      -0.110       0.051\npblack              0.0494      0.040      1.225      0.221      -0.030       0.129\npage18_39           0.0807      0.046      1.762      0.078      -0.009       0.170\nave_hh_sz          -0.0016      0.013     -0.118      0.906      -0.028       0.025\nmedian_hhincome  5.529e-07   2.85e-07      1.939      0.052   -5.93e-09    1.11e-06\npowner             -0.0190      0.033     -0.572      0.568      -0.084       0.046\npsch_atlstba       -0.0446      0.033     -1.362      0.173      -0.109       0.020\npop_propurban      -0.0323      0.013     -2.503      0.012      -0.058      -0.007\n==============================================================================\nOmnibus:                   107892.795   Durbin-Watson:                   2.001\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             5180.478\nSkew:                          -0.088   Prob(JB):                         0.00\nKurtosis:                       1.015   Cond. No.                     1.55e+06\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 1.55e+06. This might indicate that there are\nstrong multicollinearity or other numerical problems.\nIntercept          5.521342e-01\npage18_39          8.066676e-02\ncouple            -5.119242e-02\npblack             4.944180e-02\npsch_atlstba      -4.455806e-02\npop_propurban     -3.227529e-02\npwhite            -2.929282e-02\npowner            -1.903279e-02\nave_hh_sz         -1.570735e-03\nfemale             6.760573e-04\nmedian_hhincome    5.529393e-07\ndtype: float64\n\n\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                dormant   R-squared:                       0.002\nModel:                            OLS   Adj. R-squared:                  0.001\nMethod:                 Least Squares   F-statistic:                     2.579\nDate:                Mon, 29 Apr 2024   Prob (F-statistic):            0.00405\nTime:                        16:09:41   Log-Likelihood:                -11328.\nNo. Observations:               15645   AIC:                         2.268e+04\nDf Residuals:                   15634   BIC:                         2.276e+04\nDf Model:                          10                                         \nCovariance Type:            nonrobust                                         \n===================================================================================\n                      coef    std err          t      P&gt;|t|      [0.025      0.975]\n-----------------------------------------------------------------------------------\nIntercept           0.4533      0.075      6.073      0.000       0.307       0.600\nfemale             -0.0078      0.009     -0.862      0.389      -0.026       0.010\ncouple             -0.0387      0.014     -2.743      0.006      -0.066      -0.011\npwhite              0.1597      0.057      2.780      0.005       0.047       0.272\npblack              0.1424      0.056      2.546      0.011       0.033       0.252\npage18_39          -0.0007      0.065     -0.011      0.991      -0.128       0.127\nave_hh_sz           0.0120      0.019      0.637      0.524      -0.025       0.049\nmedian_hhincome  5.832e-07   4.07e-07      1.432      0.152   -2.15e-07    1.38e-06\npowner             -0.1334      0.047     -2.857      0.004      -0.225      -0.042\npsch_atlstba       -0.0696      0.047     -1.493      0.135      -0.161       0.022\npop_propurban      -0.0159      0.018     -0.882      0.378      -0.051       0.019\n==============================================================================\nOmnibus:                    54058.622   Durbin-Watson:                   2.013\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             2590.424\nSkew:                          -0.085   Prob(JB):                         0.00\nKurtosis:                       1.014   Cond. No.                     1.54e+06\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 1.54e+06. This might indicate that there are\nstrong multicollinearity or other numerical problems.\nIntercept          4.533437e-01\npwhite             1.597181e-01\npblack             1.424140e-01\npowner            -1.333556e-01\npsch_atlstba      -6.961300e-02\ncouple            -3.865036e-02\npop_propurban     -1.593688e-02\nave_hh_sz          1.201257e-02\nfemale            -7.803247e-03\npage18_39         -7.272653e-04\nmedian_hhincome    5.831812e-07\ndtype: float64"
  },
  {
    "objectID": "projects/Assignment1/hw1_questions.html#experimental-results",
    "href": "projects/Assignment1/hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\nI compared the means of these variables between the two groups. The t-test results came back with high p-values, well above the 0.05 threshold, indicating no significant differences. For instance, the p-value for the ‘mrm2’ variable was 0.905, which is much higher than the 0.05 cut-off for statistical significance. This suggests that the treatment and control groups are indeed balanced in terms of the months since the last donation.\nSimilarly, when running linear regressions, I included the treatment as an independent variable and assessed the influence of the treatment on the ‘mrm2’ variable. The coefficient on the treatment variable in these regressions was not significant, mirroring the t-test results and providing additional confirmation that the groups were balanced.\nComparing these findings with Table 1 in the paper, I noted that my analysis corroborates the original study’s success in creating equivalent groups. Table 1 in Karlan and List’s paper likely serves to demonstrate the balance across several characteristics of the donors before receiving any treatment, which is a crucial step to ensure that the effects measured are attributable to the intervention (different fundraising letters) and not confounded by pre-existing disparities.\nIn summary, both the t-tests and linear regression analyses in my replication study align closely, indicating no significant differences between the treatment and control groups for the selected variables. This suggests a successful randomization process, which allows for a robust comparison of the treatments’ effects, reinforcing the credibility of the findings, much like in the original study by Karlan and List.\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\n\n\n\n\n\n\n\n\n\n(3.101361000543946,\n 0.0019274025949016986,\n &lt;class 'statsmodels.iolib.summary.Summary'&gt;\n \"\"\"\n                             OLS Regression Results                            \n ==============================================================================\n Dep. Variable:                   gave   R-squared:                       0.000\n Model:                            OLS   Adj. R-squared:                  0.000\n Method:                 Least Squares   F-statistic:                     9.618\n Date:                Mon, 29 Apr 2024   Prob (F-statistic):            0.00193\n Time:                        16:09:41   Log-Likelihood:                 26630.\n No. Observations:               50083   AIC:                        -5.326e+04\n Df Residuals:                   50081   BIC:                        -5.324e+04\n Df Model:                           1                                         \n Covariance Type:            nonrobust                                         \n ==============================================================================\n                  coef    std err          t      P&gt;|t|      [0.025      0.975]\n ------------------------------------------------------------------------------\n const          0.0179      0.001     16.225      0.000       0.016       0.020\n treatment      0.0042      0.001      3.101      0.002       0.002       0.007\n ==============================================================================\n Omnibus:                    59814.280   Durbin-Watson:                   2.005\n Prob(Omnibus):                  0.000   Jarque-Bera (JB):          4317152.727\n Skew:                           6.740   Prob(JB):                         0.00\n Kurtosis:                      46.440   Cond. No.                         3.23\n ==============================================================================\n \n Notes:\n [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n \"\"\")\n\n\nTo understand the impact of these letters, we needed to see if the promise of matched funding would nudge more people to open their wallets. So, we ran a t-test and a linear regression, focusing on whether any charitable donation was made.\nOur bar plot showcases the proportion of people who donated in each group, and here’s where it gets interesting: both bars hover around the same height, with a slight edge to the treatment group, hinting that the promise of doubling one’s donation might have slightly increased the likelihood of giving.\nHowever, the numbers speak louder when we crunch them. The t-test, a statistical scalpel, cut through the data and revealed a p-value of 0.002, a whisper of a number that shouts “This is not random!” The linear regression, painting a broader picture, echoed this sentiment, showing that being in the treatment group had a positive effect on the decision to donate.\nIt seems that the idea of a matching grant works somewhat like a charm, convincing more people to donate. It’s not a siren’s call, leading every passerby astray, but more like a friend’s gentle nudge in the right direction. It seems that when people know their donation will be doubled, they’re slightly more inclined to contribute.\nThis doesn’t mean everyone is swayed by a matching offer. Some are, some aren’t. But on the whole, more people join the bandwagon when they know their money makes a bigger splash. This is a valuable insight for organizations relying on public generosity. It appears that even in the complex tapestry of human decision-making, a little incentive goes a long way.\n\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n                          Probit Regression Results                           \n==============================================================================\nDep. Variable:                   gave   No. Observations:                50083\nModel:                         Probit   Df Residuals:                    50081\nMethod:                           MLE   Df Model:                            1\nDate:                Mon, 29 Apr 2024   Pseudo R-squ.:               0.0009783\nTime:                        16:09:42   Log-Likelihood:                -5030.5\nconverged:                       True   LL-Null:                       -5035.4\nCovariance Type:            nonrobust   LLR p-value:                  0.001696\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -2.1001      0.023    -90.073      0.000      -2.146      -2.054\ntreatment      0.0868      0.028      3.113      0.002       0.032       0.141\n==============================================================================\n\n\nLikely the offer of a matching donation mentioned in the prompt—has a positive and significant effect on the likelihood of a participant making a donation. This aligns with the results shown in Table 3, column 1 of Karlan and List’s paper, confirming that their findings about the effectiveness of matching offers in stimulating charitable donations are replicated in your analysis.\nIn layman’s terms, this suggests that when potential donors were told their donations would be matched, they were more inclined to contribute. It’s a bit like having a buy-one-get-one-free offer; knowing that your donation will have double the impact might just be the push needed to move from intention to action. ### Differences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\ndonations_ratio1 = data[data['ratio'] == 1]['gave']\ndonations_ratio2 = data[data['ratio'] == 2]['gave']\ndonations_ratio3 = data[data['ratio'] == 3]['gave']\n\nt_test_1_vs_2 = ttest_ind(donations_ratio1, donations_ratio2)\nt_test_1_vs_3 = ttest_ind(donations_ratio1, donations_ratio3)\nt_test_2_vs_3 = ttest_ind(donations_ratio2, donations_ratio3)\n\nt_test_1_vs_2, t_test_1_vs_3, t_test_2_vs_3\n\n(TtestResult(statistic=-0.96504713432247, pvalue=0.33453168549723933, df=22265.0),\n TtestResult(statistic=-1.0150255853798622, pvalue=0.31010466370866724, df=22260.0),\n TtestResult(statistic=-0.05011583793874515, pvalue=0.9600305283739325, df=22261.0))\n\n\ntodo: Use a series of t-tests to test whether the size of the match ratio has an effect on whether people donate or not. For example, does the 2:1 match rate lead increase the likelihood that someone donates as compared to the 1:1 match rate? Do your results support the “figures suggest” comment the authors make on page 8?\n\nmodel_categorical = sm.OLS(y, X_categorical)\nresults_categorical = model_categorical.fit()\n\nprint(results_categorical.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     3.665\nDate:                Mon, 29 Apr 2024   Prob (F-statistic):             0.0118\nTime:                        16:09:42   Log-Likelihood:                 26630.\nNo. Observations:               50083   AIC:                        -5.325e+04\nDf Residuals:                   50079   BIC:                        -5.322e+04\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.0207      0.001     15.398      0.000       0.018       0.023\n2              0.0019      0.002      0.989      0.323      -0.002       0.006\n3              0.0020      0.002      1.041      0.298      -0.002       0.006\nControl       -0.0029      0.002     -1.661      0.097      -0.006       0.001\n==============================================================================\nOmnibus:                    59812.754   Durbin-Watson:                   2.005\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          4316693.217\nSkew:                           6.740   Prob(JB):                         0.00\nKurtosis:                      46.438   Cond. No.                         5.09\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\ntodo: Assess the same issue using a regression. Specifically, create the variable ratio1 then regress gave on ratio1, ratio2, and ratio3 (or alternatively, regress gave on the categorical variable ratio). Interpret the coefficients and their statistical precision.\ntodo: Calculate the response rate difference between the 1:1 and 2:1 match ratios and the 2:1 and 3:1 ratios. Do this directly from the data, and do it by computing the differences in the fitted coefficients of the previous regression. what do you conclude regarding the effectiveness of different sizes of matched donations?\n\n# Define the ratios to be analyzed\nratios = ['1', '2', '3']\n\n# Calculate donation rates for each ratio\ndonation_rates = {ratio: data[data['ratio'] == ratio]['gave'].mean() for ratio in ratios}\n\n# Calculate differences between successive donation rates\nrate_differences = {f\"{ratios[i]}_to_{ratios[i+1]}\": donation_rates[ratios[i+1]] - donation_rates[ratios[i]]\n                    for i in range(len(ratios) - 1)}\n\n# Enhanced print statements with more detailed messaging\nfor diff_label, diff_value in rate_differences.items():\n    # Formatting the label for more readable output\n    ratio_pair = diff_label.replace('_', ' to ')\n    print(f\"Change in donation rates from {ratio_pair.replace(':', ' to ')} ratio: {diff_value:.6f}\")\n\nChange in donation rates from 1 to to to 2 ratio: 0.001884\nChange in donation rates from 2 to to to 3 ratio: 0.000100\n\n\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\ntodo: Calculate a t-test or run a bivariate linear regression of the donation amount on the treatment status. What do we learn from doing this analysis?\n\n\nT-Statistic: 1.9182618934467577\nP-Value: 0.05508566528918335\n\n\ntodo: now limit the data to just people who made a donation and repeat the previous analysis. This regression allows you to analyze how much respondents donate conditional on donating some positive amount. Interpret the regression coefficients – what did we learn? Does the treatment coefficient have a causal interpretation?\n\n\nT-Statistic: -0.5846089794983359\nP-Value: 0.5590471865673547\n\n\ntodo: Make two plot: one for the treatment group and one for the control. Each plot should be a histogram of the donation amounts only among people who donated. Add a red vertical bar or some other annotation to indicate the sample average for each plot.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set the style of seaborn for more attractive and informative graphics\nsns.set(style=\"whitegrid\")\n\ndonors_data = data[data['amount'] &gt; 0]\n\ntreatment_donors = donors_data[donors_data['treatment'] == 1]['amount']\ncontrol_donors = donors_data[donors_data['treatment'] == 0]['amount']\n\ntreatment_mean = treatment_donors.mean()\ncontrol_mean = control_donors.mean()\n\nplt.figure(figsize=(14, 6))  # Adjusted for a better fit of both histograms\n\n# Histogram for the Treatment Group\nplt.subplot(1, 2, 1)\nsns.histplot(treatment_donors, bins=30, color='green', kde=True, edgecolor='black')\nplt.axvline(treatment_mean, color='darkred', linestyle='dashed', linewidth=2)\nplt.title('Treatment Group Donation Amounts')\nplt.xlabel('Donation Amount ($)')\nplt.ylabel('Frequency')\nplt.annotate(f'Mean: {treatment_mean:.2f}', xy=(treatment_mean, 10), xytext=(treatment_mean + 50, 12),\n             arrowprops=dict(facecolor='black', arrowstyle='-&gt;'))\n\n# Histogram for the Control Group\nplt.subplot(1, 2, 2)\nsns.histplot(control_donors, bins=30, color='purple', kde=True, edgecolor='black')\nplt.axvline(control_mean, color='darkred', linestyle='dashed', linewidth=2)\nplt.title('Control Group Donation Amounts')\nplt.xlabel('Donation Amount ($)')\nplt.ylabel('Frequency')\nplt.annotate(f'Mean: {control_mean:.2f}', xy=(control_mean, 10), xytext=(control_mean + 50, 12),\n             arrowprops=dict(facecolor='black', arrowstyle='-&gt;'))\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "projects/Assignment1/hw1_questions.html#simulation-experiment",
    "href": "projects/Assignment1/hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\nto do: Make a plot like those on slide 43 from our first class and explain the plot to the reader. To do this, you will simulate 100,00 draws from the control distribution and 10,000 draws from the treatment distribution. You’ll then calculate a vector of 10,000 differences, and then you’ll plot the cumulative average of that vector of differences. Comment on whether the cumulative average approaches the true difference in means.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Parameters set by the professor\np_control = 0.018\np_treatment = 0.022\nn_simulations = 10000\n\n# Simulating outcomes\ncontrol_draws = np.random.binomial(1, p_control, n_simulations)\ntreatment_draws = np.random.binomial(1, p_treatment, n_simulations)\n\n# Calculating differences between treatment and control outcomes\ndiff_vector = treatment_draws - control_draws\n\n# Calculating cumulative average of differences over simulations\ncumulative_avg_diff = np.cumsum(diff_vector) / np.arange(1, n_simulations + 1)\n\n# Setting up the plot\nplt.figure(figsize=(10, 6))\nplt.plot(cumulative_avg_diff, color='blue', label='Cumulative Average of Differences')\nplt.axhline((p_treatment - p_control), color='red', linestyle='--', label='Expected Difference')\nplt.xlabel('Number of Simulations')\nplt.ylabel('Cumulative Average Difference')\nplt.title('Simulation of Difference Between Treatment and Control')\nplt.legend()\nplt.grid(True)  # Adding a grid for better readability of the plot\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCentral Limit Theorem\nto do: Make 4 histograms like those on slide 44 from our first class at sample sizes 50, 200, 500, and 1000 and explain these plots to the reader. To do this for a sample size of e.g. 50, take 50 draws from each of the control and treatment distributions, and calculate the average difference between those draws. Then repeat that process 999 more times so that you have 1000 averages. Plot the histogram of those averages. Comment on whether zero is in the “middle” of the distribution or whether it’s in the “tail.”\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef simulate_and_plot_histograms(control_prob, treatment_prob, sample_sizes, num_repetitions=1000):\n    # Setting up the visual style using seaborn for more appealing histograms\n    sns.set(style=\"whitegrid\", palette=\"pastel\")\n\n    fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n    fig.suptitle('Distribution of Difference in Proportions Across Sample Sizes')\n\n    for i, sample_size in enumerate(sample_sizes):\n        mean_differences = []\n\n        for _ in range(num_repetitions):\n            control_draws = np.random.binomial(1, control_prob, sample_size)\n            treatment_draws = np.random.binomial(1, treatment_prob, sample_size)\n            mean_differences.append(treatment_draws.mean() - control_draws.mean())\n\n        # Using seaborn for histogram plotting for improved aesthetics\n        sns.histplot(mean_differences, bins=30, kde=True, color='skyblue', ax=axes[i])\n        axes[i].axvline(0, color='red', linestyle='dashed', linewidth=2, label='No Difference')\n        axes[i].axvline(treatment_prob - control_prob, color='green', linestyle='dashed', linewidth=2, label='True Difference')\n        axes[i].set_title(f'Sample Size: {sample_size}')\n        axes[i].set_xlabel('Mean Difference')\n        axes[i].set_ylabel('Frequency')\n        if i == 0:  # Adding the legend to only the first subplot for clarity\n            axes[i].legend()\n\n    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n    plt.show()\n\n# Control and treatment probabilities given\ncontrol_prob = 0.018\ntreatment_prob = 0.022\n\n# List of sample sizes to simulate\nsample_sizes = [50, 200, 500, 1000]\n\n# Call the function with the updated parameters and visual setup\nsimulate_and_plot_histograms(control_prob, treatment_prob, sample_sizes)"
  },
  {
    "objectID": "projects/Assignment2/hw2_questions.html",
    "href": "projects/Assignment2/hw2_questions.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\n\n\n\n\n\n\n\n\nUnnamed: 0\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n1\n0\nMidwest\n32.5\n0\n\n\n1\n786\n3\nSouthwest\n37.5\n0\n\n\n2\n348\n4\nNorthwest\n27.0\n1\n\n\n3\n927\n3\nNortheast\n24.5\n0\n\n\n4\n830\n3\nSouthwest\n37.0\n0\n\n\n...\n...\n...\n...\n...\n...\n\n\n1495\n1366\n2\nNortheast\n18.5\n1\n\n\n1496\n619\n3\nSouthwest\n22.5\n0\n\n\n1497\n826\n4\nSouthwest\n17.0\n0\n\n\n1498\n601\n3\nSouth\n29.0\n0\n\n\n1499\n602\n1\nSouth\n39.0\n0\n\n\n\n\n1500 rows × 5 columns\n\n\n\n\n# Drop the 'Unnamed: 0' column\nblueprinty = blueprinty.drop(columns=['Unnamed: 0'])\nblueprinty\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n...\n...\n...\n...\n...\n\n\n1495\n2\nNortheast\n18.5\n1\n\n\n1496\n3\nSouthwest\n22.5\n0\n\n\n1497\n4\nSouthwest\n17.0\n0\n\n\n1498\n3\nSouth\n29.0\n0\n\n\n1499\n1\nSouth\n39.0\n0\n\n\n\n\n1500 rows × 4 columns\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMean Number of Patents for Non-Customers: 3.62\n\n\nThe histogram for non-customers reveals a right-skewed distribution of patent counts, where a significant majority of non-customers have fewer patents, but there are outliers with higher numbers. This skewness suggests that while few non-customers are very innovative, most maintain a lower profile in terms of patent production. The mean number of patents for non-customers is approximately 3.62, underscoring the fact that non-customers generally have fewer patents.\n\n\n\n\n\n\n\n\n\nMean Number of Patents for Customers: 4.09\n\n\nIn contrast, the histogram for customers also shows a right-skewed distribution but with a noticeable shift towards higher counts of patents. This indicates that customers are generally more active in patenting than non-customers. The mean number of patents for customers, at approximately 4.09, is higher than that of non-customers. This might imply that customer status could be associated with higher innovation levels or that entities with higher patent activities are more likely to be customers.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n\n&lt;Figure size 768x576 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n(26.691481197237145,\n 24.1497461928934,\n region      Midwest  Northeast  Northwest  South  Southwest\n iscustomer                                                 \n 0               207        488        171    171        266\n 1                17        113         16     20         31)\n\n\n\n\n\nThe Northeast region has the highest overall number of entities, both customers and non-customers.\nCustomers form a smaller proportion of the total in each region, but the disparity in numbers between customers and non-customers varies by region.\n\nThe younger average age of customers might indicate that younger entities (or individuals) are more likely to become customers, possibly due to newer businesses being more inclined to engage with the offerings. The regional analysis shows that while all regions have more non-customers than customers, the Northeast stands out with a relatively higher number of customers, suggesting regional variations in customer acquisition or market penetration strategies.\nThese insights could be used to tailor regional marketing strategies or to explore further why younger demographics are more represented among customers.\n\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\ntodo: Write down mathematically the likelihood for \\(Y \\sim \\text{Poisson}(\\lambda)\\). Note that \\(f(Y|\\lambda) = e^{-\\lambda}\\lambda^Y/Y!\\).\npoisson_loglikelihood &lt;- function(lambda, Y){\n   ...\n}\nUse the function to plot lambda on the horizontal axis and the likelihood (or log-likelihood) on the vertical axis for a range of lambdas (use the observed number of patents as the input for Y)\n\n\n\n\n\n\n\n\n\n\n\n\nShape: The curve typically shows a peak, indicating the value of λ that maximizes the log-likelihood. This value can be considered as the most likely estimate of λ given the observed data.\n\n\n\n\n\nThe peak of the log-likelihood curve provides an insight into the most probable rate of patents per firm over the last 5 years, assuming a Poisson distribution. This graphical analysis is useful in understanding the behavior of the likelihood as λ changes and helps in choosing an appropriate λ for further statistical analysis or modeling.\n\ntodo: If you’re feeling mathematical, take the first derivative of your likelihood or log-likelihood, set it equal to zero and solve for lambda. You will find lambda_mle is Ybar, which “feels right” because the mean of a Poisson distribution is lambda.\n\nTo derive the Maximum Likelihood Estimator (MLE) for λ in a Poisson distribution, we begin with the log-likelihood function and find the value of λ that maximizes this function. The log-likelihood for the Poisson distribution given Y and λ is:\n\n\nℓ(λ) = ∑i=1n (-λ + Yi · log(λ) - log(Yi!))\n\n\nWhere n is the number of observations.\n\n\n\nTo find the maximum, we take the derivative of the log-likelihood with respect to λ and set it to zero. The derivative is:\n\n\ndℓ/dλ = ∑i=1n (-1 + Yi / λ)\n\n\nSetting this derivative equal to zero for maximization:\n\n\n-n + ∑i=1n Yi / λ = 0\n\n\nλ = (∑i=1n Yi) / n\n\n\n\nThis result shows that the MLE for λ, λMLE, is the sample mean ̌Y of the observed data:\n\n\nλMLE = ̌Y\n\n\nThis is intuitively satisfying as the mean of a Poisson distribution is λ, and the MLE estimates the parameter such that the observed mean is the most likely estimate under the assumed model.\n\n\n# Calculate lambda MLE, which should be the mean of Y\nlambda_mle = Y.mean()\n\nlambda_mle\n\n3.6846666666666668\n\n\n\nThe calculated Maximum Likelihood Estimate (MLE) for λ, which is λMLE, is approximately 3.685. This confirms our derivation: the MLE of λ for a Poisson distribution is indeed the sample mean of the observed counts Y, representing the average number of patents awarded per firm over the last 5 years.\n\n\n\n(3.684666666666663, True)\n\n\n\n\nWe used a Poisson regression model to understand the distribution of patents across different engineering firms over the last 5 years. The Poisson model is appropriate here because the number of patents is count data, typically non-negative integers, and we’re considering an interval of time.\n\n\nThe Poisson distribution is characterized by its rate parameter, λ, which represents the average number of events (patents) in a given time frame. The key property of a Poisson distribution is that its mean and variance are both equal to λ.\n\n\nWe aimed to estimate this λ using the method of Maximum Likelihood Estimation (MLE). The MLE is a statistical method for estimating the parameters of a model. It works by finding the parameter values that make the observed data most probable.\n\n\n\nWe derived mathematically that the MLE for λ is the sample mean (̌Y) of our observed patent counts. This derivation was based on setting the first derivative of the log-likelihood function to zero, solving for λ, and demonstrating that λMLE equates to the sample mean.\n\n\n\nWe then used numerical optimization to confirm this result. Because Python’s optimization functions typically minimize rather than maximize, we minimized the negative of the log-likelihood function. The result from the scipy.optimize function confirmed that the estimated λ is indeed approximately equal to the sample mean, which validates our earlier mathematical derivation.\n\n\n\nThe interpretation of this result is that the most likely average rate of patent awards across all engineering firms is around 3.685 patents per firm over the last 5 years. This is an intuitive result because, in a Poisson distribution, the rate λ is the expected count per interval. Therefore, estimating λ as the average observed count aligns with our understanding of the distribution’s properties.\n\n\nMoreover, this value of λ could be used to predict the expected number of patent awards for similar engineering firms, under similar conditions, over a 5-year period. It also serves as a benchmark for comparing individual firm performance against the average.\n\n\nIn practice, this Poisson model might be the basis for more complex analyses, such as Poisson regression models that relate λ to other explanatory variables (e.g., firm size, region, R&D spending) to better understand the factors that influence patent output.\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\nUpdating the likelihood or log-likelihood function with an additional argument to take in a covariate matrix X. Also change the parameter of the model from lambda to the beta vector. In this model, lambda must be a positive number, so we choose the inverse link function g() to be exp() so that_ \\(\\lambda_i = e^{X_i'\\beta}\\). For Example:\npoisson_regression_likelihood &lt;- function(beta, Y, X){\n   ...\n}\n\n\nColumns in dataframe: Index(['patents', 'age', 'iscustomer', 'constant', 'age_squared', 'Northeast',\n       'Northwest', 'South', 'Southwest'],\n      dtype='object')\n\n\n\n\n\n\nFunction Value (F): The final function value, F=3275.85, represents the negative log-likelihood at the optimal parameters. The lower this value, the better the fit of the model to your data, assuming the model structure is appropriate.\nProjected Gradient (Projg): The norm of the final projected gradient being 5.543×10−25.543 \\times 10^{-2}5.543×10−2 is quite small. This suggests that the optimization algorithm has effectively minimized the function, as changes in the parameter values would no longer significantly decrease the function value.\nConvergence Message: “CONVERGENCE: REL_REDUCTION_OF_F_&lt;=_FACTR*EPSMCH” indicates that the relative reduction in the function value has reached a threshold that is considered sufficient for convergence, based on the precision of the machine. This means the optimization has likely found a stable point that is close to the true minimum.\n\n\n\n\nThe coefficients you obtained:\n\nIntercept (Constant): 1.215\nAge: 1.046\nAge Squared: −1.141\nIs Customer: 0.118\nRegion Variables: These include slight adjustments for regions which seem to have small but potentially meaningful impacts compared to a baseline region (omitted in your dummy coding).\n\n\n\n\n\nAge and Age Squared: The positive coefficient for age and the negative coefficient for age squared suggest a quadratic relationship. Initially, the number of patents increases with age but starts to decrease as age squared becomes more influential. This could indicate that there’s an optimal age for productivity in terms of patents, beyond which it declines.\nIs Customer: The positive coefficient for being a customer (0.118) implies that customers, on average, have a higher expected count of patents, holding other factors constant. This could be interpreted as a beneficial effect of being a customer on patent output.\nRegional Effects: The coefficients for the regions adjust the baseline expectation of patent counts relative to the omitted region. These effects are relatively small, indicating that while there are regional differences, they are not as pronounced as the effects of age or customer status."
  },
  {
    "objectID": "projects/Assignment2/hw2_questions.html#blueprinty-case-study",
    "href": "projects/Assignment2/hw2_questions.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\n\n\n\n\n\n\n\n\nUnnamed: 0\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n1\n0\nMidwest\n32.5\n0\n\n\n1\n786\n3\nSouthwest\n37.5\n0\n\n\n2\n348\n4\nNorthwest\n27.0\n1\n\n\n3\n927\n3\nNortheast\n24.5\n0\n\n\n4\n830\n3\nSouthwest\n37.0\n0\n\n\n...\n...\n...\n...\n...\n...\n\n\n1495\n1366\n2\nNortheast\n18.5\n1\n\n\n1496\n619\n3\nSouthwest\n22.5\n0\n\n\n1497\n826\n4\nSouthwest\n17.0\n0\n\n\n1498\n601\n3\nSouth\n29.0\n0\n\n\n1499\n602\n1\nSouth\n39.0\n0\n\n\n\n\n1500 rows × 5 columns\n\n\n\n\n# Drop the 'Unnamed: 0' column\nblueprinty = blueprinty.drop(columns=['Unnamed: 0'])\nblueprinty\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n...\n...\n...\n...\n...\n\n\n1495\n2\nNortheast\n18.5\n1\n\n\n1496\n3\nSouthwest\n22.5\n0\n\n\n1497\n4\nSouthwest\n17.0\n0\n\n\n1498\n3\nSouth\n29.0\n0\n\n\n1499\n1\nSouth\n39.0\n0\n\n\n\n\n1500 rows × 4 columns\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMean Number of Patents for Non-Customers: 3.62\n\n\nThe histogram for non-customers reveals a right-skewed distribution of patent counts, where a significant majority of non-customers have fewer patents, but there are outliers with higher numbers. This skewness suggests that while few non-customers are very innovative, most maintain a lower profile in terms of patent production. The mean number of patents for non-customers is approximately 3.62, underscoring the fact that non-customers generally have fewer patents.\n\n\n\n\n\n\n\n\n\nMean Number of Patents for Customers: 4.09\n\n\nIn contrast, the histogram for customers also shows a right-skewed distribution but with a noticeable shift towards higher counts of patents. This indicates that customers are generally more active in patenting than non-customers. The mean number of patents for customers, at approximately 4.09, is higher than that of non-customers. This might imply that customer status could be associated with higher innovation levels or that entities with higher patent activities are more likely to be customers.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n\n&lt;Figure size 768x576 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n(26.691481197237145,\n 24.1497461928934,\n region      Midwest  Northeast  Northwest  South  Southwest\n iscustomer                                                 \n 0               207        488        171    171        266\n 1                17        113         16     20         31)\n\n\n\n\n\nThe Northeast region has the highest overall number of entities, both customers and non-customers.\nCustomers form a smaller proportion of the total in each region, but the disparity in numbers between customers and non-customers varies by region.\n\nThe younger average age of customers might indicate that younger entities (or individuals) are more likely to become customers, possibly due to newer businesses being more inclined to engage with the offerings. The regional analysis shows that while all regions have more non-customers than customers, the Northeast stands out with a relatively higher number of customers, suggesting regional variations in customer acquisition or market penetration strategies.\nThese insights could be used to tailor regional marketing strategies or to explore further why younger demographics are more represented among customers.\n\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\ntodo: Write down mathematically the likelihood for \\(Y \\sim \\text{Poisson}(\\lambda)\\). Note that \\(f(Y|\\lambda) = e^{-\\lambda}\\lambda^Y/Y!\\).\npoisson_loglikelihood &lt;- function(lambda, Y){\n   ...\n}\nUse the function to plot lambda on the horizontal axis and the likelihood (or log-likelihood) on the vertical axis for a range of lambdas (use the observed number of patents as the input for Y)\n\n\n\n\n\n\n\n\n\n\n\n\nShape: The curve typically shows a peak, indicating the value of λ that maximizes the log-likelihood. This value can be considered as the most likely estimate of λ given the observed data.\n\n\n\n\n\nThe peak of the log-likelihood curve provides an insight into the most probable rate of patents per firm over the last 5 years, assuming a Poisson distribution. This graphical analysis is useful in understanding the behavior of the likelihood as λ changes and helps in choosing an appropriate λ for further statistical analysis or modeling.\n\ntodo: If you’re feeling mathematical, take the first derivative of your likelihood or log-likelihood, set it equal to zero and solve for lambda. You will find lambda_mle is Ybar, which “feels right” because the mean of a Poisson distribution is lambda.\n\nTo derive the Maximum Likelihood Estimator (MLE) for λ in a Poisson distribution, we begin with the log-likelihood function and find the value of λ that maximizes this function. The log-likelihood for the Poisson distribution given Y and λ is:\n\n\nℓ(λ) = ∑i=1n (-λ + Yi · log(λ) - log(Yi!))\n\n\nWhere n is the number of observations.\n\n\n\nTo find the maximum, we take the derivative of the log-likelihood with respect to λ and set it to zero. The derivative is:\n\n\ndℓ/dλ = ∑i=1n (-1 + Yi / λ)\n\n\nSetting this derivative equal to zero for maximization:\n\n\n-n + ∑i=1n Yi / λ = 0\n\n\nλ = (∑i=1n Yi) / n\n\n\n\nThis result shows that the MLE for λ, λMLE, is the sample mean ̌Y of the observed data:\n\n\nλMLE = ̌Y\n\n\nThis is intuitively satisfying as the mean of a Poisson distribution is λ, and the MLE estimates the parameter such that the observed mean is the most likely estimate under the assumed model.\n\n\n# Calculate lambda MLE, which should be the mean of Y\nlambda_mle = Y.mean()\n\nlambda_mle\n\n3.6846666666666668\n\n\n\nThe calculated Maximum Likelihood Estimate (MLE) for λ, which is λMLE, is approximately 3.685. This confirms our derivation: the MLE of λ for a Poisson distribution is indeed the sample mean of the observed counts Y, representing the average number of patents awarded per firm over the last 5 years.\n\n\n\n(3.684666666666663, True)\n\n\n\n\nWe used a Poisson regression model to understand the distribution of patents across different engineering firms over the last 5 years. The Poisson model is appropriate here because the number of patents is count data, typically non-negative integers, and we’re considering an interval of time.\n\n\nThe Poisson distribution is characterized by its rate parameter, λ, which represents the average number of events (patents) in a given time frame. The key property of a Poisson distribution is that its mean and variance are both equal to λ.\n\n\nWe aimed to estimate this λ using the method of Maximum Likelihood Estimation (MLE). The MLE is a statistical method for estimating the parameters of a model. It works by finding the parameter values that make the observed data most probable.\n\n\n\nWe derived mathematically that the MLE for λ is the sample mean (̌Y) of our observed patent counts. This derivation was based on setting the first derivative of the log-likelihood function to zero, solving for λ, and demonstrating that λMLE equates to the sample mean.\n\n\n\nWe then used numerical optimization to confirm this result. Because Python’s optimization functions typically minimize rather than maximize, we minimized the negative of the log-likelihood function. The result from the scipy.optimize function confirmed that the estimated λ is indeed approximately equal to the sample mean, which validates our earlier mathematical derivation.\n\n\n\nThe interpretation of this result is that the most likely average rate of patent awards across all engineering firms is around 3.685 patents per firm over the last 5 years. This is an intuitive result because, in a Poisson distribution, the rate λ is the expected count per interval. Therefore, estimating λ as the average observed count aligns with our understanding of the distribution’s properties.\n\n\nMoreover, this value of λ could be used to predict the expected number of patent awards for similar engineering firms, under similar conditions, over a 5-year period. It also serves as a benchmark for comparing individual firm performance against the average.\n\n\nIn practice, this Poisson model might be the basis for more complex analyses, such as Poisson regression models that relate λ to other explanatory variables (e.g., firm size, region, R&D spending) to better understand the factors that influence patent output.\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\nUpdating the likelihood or log-likelihood function with an additional argument to take in a covariate matrix X. Also change the parameter of the model from lambda to the beta vector. In this model, lambda must be a positive number, so we choose the inverse link function g() to be exp() so that_ \\(\\lambda_i = e^{X_i'\\beta}\\). For Example:\npoisson_regression_likelihood &lt;- function(beta, Y, X){\n   ...\n}\n\n\nColumns in dataframe: Index(['patents', 'age', 'iscustomer', 'constant', 'age_squared', 'Northeast',\n       'Northwest', 'South', 'Southwest'],\n      dtype='object')\n\n\n\n\n\n\nFunction Value (F): The final function value, F=3275.85, represents the negative log-likelihood at the optimal parameters. The lower this value, the better the fit of the model to your data, assuming the model structure is appropriate.\nProjected Gradient (Projg): The norm of the final projected gradient being 5.543×10−25.543 \\times 10^{-2}5.543×10−2 is quite small. This suggests that the optimization algorithm has effectively minimized the function, as changes in the parameter values would no longer significantly decrease the function value.\nConvergence Message: “CONVERGENCE: REL_REDUCTION_OF_F_&lt;=_FACTR*EPSMCH” indicates that the relative reduction in the function value has reached a threshold that is considered sufficient for convergence, based on the precision of the machine. This means the optimization has likely found a stable point that is close to the true minimum.\n\n\n\n\nThe coefficients you obtained:\n\nIntercept (Constant): 1.215\nAge: 1.046\nAge Squared: −1.141\nIs Customer: 0.118\nRegion Variables: These include slight adjustments for regions which seem to have small but potentially meaningful impacts compared to a baseline region (omitted in your dummy coding).\n\n\n\n\n\nAge and Age Squared: The positive coefficient for age and the negative coefficient for age squared suggest a quadratic relationship. Initially, the number of patents increases with age but starts to decrease as age squared becomes more influential. This could indicate that there’s an optimal age for productivity in terms of patents, beyond which it declines.\nIs Customer: The positive coefficient for being a customer (0.118) implies that customers, on average, have a higher expected count of patents, holding other factors constant. This could be interpreted as a beneficial effect of being a customer on patent output.\nRegional Effects: The coefficients for the regions adjust the baseline expectation of patent counts relative to the omitted region. These effects are relatively small, indicating that while there are regional differences, they are not as pronounced as the effects of age or customer status."
  },
  {
    "objectID": "projects/Assignment2/hw2_questions.html#airbnb-case-study",
    "href": "projects/Assignment2/hw2_questions.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\ntodo: Assume the number of reviews is a good proxy for the number of bookings. Perform some exploratory data analysis to get a feel for the data, handle or drop observations with missing values on relevant variables, build one or more models (e.g., a poisson regression model for the number of bookings as proxied by the number of reviews), and interpret model coefficients to describe variation in the number of reviews as a function of the variables provided.\n\n\n\n\n\n\n\n\n\nid\ndays\nlast_scraped\nhost_since\nroom_type\nbathrooms\nbedrooms\nprice\nnumber_of_reviews\nreview_scores_cleanliness\nreview_scores_location\nreview_scores_value\ninstant_bookable\n\n\n\n\n0\n2515\n3130\n4/2/2017\n9/6/2008\nPrivate room\n1.0\n1.0\n59\n150\n9.0\n9.0\n9.0\nf\n\n\n1\n2595\n3127\n4/2/2017\n9/9/2008\nEntire home/apt\n1.0\n0.0\n230\n20\n9.0\n10.0\n9.0\nf\n\n\n2\n3647\n3050\n4/2/2017\n11/25/2008\nPrivate room\n1.0\n1.0\n150\n0\nNaN\nNaN\nNaN\nf\n\n\n3\n3831\n3038\n4/2/2017\n12/7/2008\nEntire home/apt\n1.0\n1.0\n89\n116\n9.0\n9.0\n9.0\nf\n\n\n4\n4611\n3012\n4/2/2017\n1/2/2009\nPrivate room\nNaN\n1.0\n39\n93\n9.0\n8.0\n9.0\nt\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n40623\n18008937\n266\n4/2/2017\n7/10/2016\nEntire home/apt\n1.5\n2.0\n150\n0\nNaN\nNaN\nNaN\nt\n\n\n40624\n18009045\n366\n4/2/2017\n4/1/2016\nPrivate room\n1.0\n1.0\n125\n0\nNaN\nNaN\nNaN\nf\n\n\n40625\n18009065\n587\n4/2/2017\n8/24/2015\nPrivate room\n1.0\n1.0\n80\n0\nNaN\nNaN\nNaN\nt\n\n\n40626\n18009650\n335\n4/2/2017\n5/2/2016\nPrivate room\n1.0\n1.0\n69\n0\nNaN\nNaN\nNaN\nt\n\n\n40627\n18009669\n1\n4/2/2017\n4/1/2017\nEntire home/apt\n1.0\n1.0\n115\n0\nNaN\nNaN\nNaN\nt\n\n\n\n\n40628 rows × 13 columns\n\n\n\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 40361 entries, 0 to 40627\nData columns (total 13 columns):\n #   Column                     Non-Null Count  Dtype         \n---  ------                     --------------  -----         \n 0   id                         40361 non-null  int64         \n 1   days                       40361 non-null  int64         \n 2   last_scraped               40361 non-null  datetime64[ns]\n 3   host_since                 40361 non-null  datetime64[ns]\n 4   room_type                  40361 non-null  object        \n 5   bathrooms                  40361 non-null  float64       \n 6   bedrooms                   40361 non-null  float64       \n 7   price                      40361 non-null  int64         \n 8   number_of_reviews          40361 non-null  int64         \n 9   review_scores_cleanliness  30227 non-null  float64       \n 10  review_scores_location     30168 non-null  float64       \n 11  review_scores_value        30166 non-null  float64       \n 12  instant_bookable           40361 non-null  object        \ndtypes: datetime64[ns](2), float64(5), int64(4), object(2)\nmemory usage: 4.3+ MB\n\n\n/tmp/ipykernel_81391/241246695.py:8: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  airbnb['last_scraped'] = pd.to_datetime(airbnb['last_scraped'], errors='coerce')\n/tmp/ipykernel_81391/241246695.py:9: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  airbnb['host_since'] = pd.to_datetime(airbnb['host_since'], errors='coerce')\n\n\n(None,\n      id  days last_scraped host_since        room_type  bathrooms  bedrooms  \\\n 0  2515  3130   2017-04-02 2008-09-06     Private room        1.0       1.0   \n 1  2595  3127   2017-04-02 2008-09-09  Entire home/apt        1.0       0.0   \n 2  3647  3050   2017-04-02 2008-11-25     Private room        1.0       1.0   \n 3  3831  3038   2017-04-02 2008-12-07  Entire home/apt        1.0       1.0   \n 5  5099  2981   2017-04-02 2009-02-02  Entire home/apt        1.0       1.0   \n \n    price  number_of_reviews  review_scores_cleanliness  \\\n 0     59                150                        9.0   \n 1    230                 20                        9.0   \n 2    150                  0                        NaN   \n 3     89                116                        9.0   \n 5    212                 60                        9.0   \n \n    review_scores_location  review_scores_value instant_bookable  \n 0                     9.0                  9.0                f  \n 1                    10.0                  9.0                f  \n 2                     NaN                  NaN                f  \n 3                     9.0                  9.0                f  \n 5                     9.0                  9.0                f  )\n\n\n\n\n/tmp/ipykernel_81391/2893478340.py:24: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  airbnb['month_year'] = airbnb['last_scraped'].dt.to_period('M')\n/home/jovyan/.rsm-msba/conda/envs/myenv/lib/python3.11/site-packages/pandas/plotting/_matplotlib/core.py:1561: UserWarning: Attempting to set identical low and high xlims makes transformation singular; automatically expanding.\n  ax.set_xlim(left, right)\n\n\n\n\n\n\n\n\n\n\n\n/home/jovyan/.rsm-msba/conda/envs/myenv/lib/python3.11/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n\n\n(29.439215673370676,\n 18.257012488357216,\n array([-0.05203667, -0.10747665, -0.11195226,  0.27460204]))"
  },
  {
    "objectID": "projects/Assignment2/hw2_questions.html#airbnb-listing-analysis---poisson-regression-model-results",
    "href": "projects/Assignment2/hw2_questions.html#airbnb-listing-analysis---poisson-regression-model-results",
    "title": "Poisson Regression Examples",
    "section": "Airbnb Listing Analysis - Poisson Regression Model Results",
    "text": "Airbnb Listing Analysis - Poisson Regression Model Results\nIn this analysis, we built a Poisson regression model to predict the number of reviews for Airbnb listings, using price, room_type, and instant_bookable status as predictors. The evaluation of the model yielded a Root Mean Squared Error (RMSE) of 29.44 and a Mean Absolute Error (MAE) of 18.26, indicating a relatively large variance between the predicted and actual number of reviews. This variability suggests that the model might benefit from including additional predictors or exploring more complex modeling approaches.\n\nInterpretation of Coefficients\nThe model coefficients provided insights into the relationships between the features and the number of reviews:\n\nPrice: A coefficient of -0.052 suggests that higher prices are slightly associated with fewer reviews. This relationship might indicate that more expensive listings are less frequently booked or reviewed.\nRoom Type: The coefficients for room types (Entire home/apt: -0.107, Shared room: -0.112) compared to the baseline category (Private room) suggest that these room types are generally reviewed less frequently. This might reflect a preference or higher usage pattern for private rooms in the Airbnb platform.\nInstant Bookable: Listings that allow instant booking (coefficient of +0.275) tend to have more reviews, likely due to the ease of booking facilitating more frequent stays and consequently more reviews.\n\nThese findings highlight the importance of pricing strategy and listing features in influencing customer engagement in terms of reviews. Future work could explore additional variables, such as location, host characteristics, and special amenities, to enhance the model’s accuracy and predictive power."
  },
  {
    "objectID": "projects/Assignment3/hw3_questions.html",
    "href": "projects/Assignment3/hw3_questions.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "This assignment uses the MNL model to analyze (1) yogurt purchase data made by consumers at a retail location, and (2) conjoint data about consumer preferences for minivans."
  },
  {
    "objectID": "projects/Assignment3/hw3_questions.html#estimating-yogurt-preferences",
    "href": "projects/Assignment3/hw3_questions.html#estimating-yogurt-preferences",
    "title": "Poisson Regression Examples",
    "section": "1. Estimating Yogurt Preferences",
    "text": "1. Estimating Yogurt Preferences\n\nLikelihood for the Multi-nomial Logit (MNL) Model\nSuppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 4 products, then either \\(y=3\\) or \\(y=(0,0,1,0)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, size, price, etc.).\nWe model the consumer’s decision as the selection of the product that provides the most utility, and we’ll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 4 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta} + e^{x_4'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=\\delta_{i4}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 \\times \\mathbb{P}_i(4)^0 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]\n\n\nYogurt Dataset\nWe will use the yogurt_data dataset, which provides anonymized consumer identifiers (id), a vector indicating the chosen product (y1:y4), a vector indicating if any products were “featured” in the store as a form of advertising (f1:f4), and the products’ prices (p1:p4). For example, consumer 1 purchased yogurt 4 at a price of 0.079/oz and none of the yogurts were featured/advertised at the time of consumer 1’s purchase. Consumers 2 through 7 each bought yogurt 2, etc.\n\n\n\n\n\n\n\n\n\n\nid\ny1\ny2\ny3\ny4\nf1\nf2\nf3\nf4\np1\np2\np3\np4\n\n\n\n\n0\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0.108\n0.081\n0.061\n0.079\n\n\n1\n2\n0\n1\n0\n0\n0\n0\n0\n0\n0.108\n0.098\n0.064\n0.075\n\n\n2\n3\n0\n1\n0\n0\n0\n0\n0\n0\n0.108\n0.098\n0.061\n0.086\n\n\n3\n4\n0\n1\n0\n0\n0\n0\n0\n0\n0.108\n0.098\n0.061\n0.086\n\n\n4\n5\n0\n1\n0\n0\n0\n0\n0\n0\n0.125\n0.098\n0.049\n0.079\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2425\n2426\n0\n0\n0\n1\n0\n0\n0\n0\n0.122\n0.086\n0.061\n0.086\n\n\n2426\n2427\n0\n0\n0\n1\n0\n0\n0\n0\n0.122\n0.086\n0.061\n0.086\n\n\n2427\n2428\n0\n0\n0\n1\n0\n0\n0\n0\n0.122\n0.086\n0.061\n0.086\n\n\n2428\n2429\n0\n0\n0\n1\n0\n0\n0\n0\n0.122\n0.086\n0.050\n0.086\n\n\n2429\n2430\n0\n0\n0\n1\n0\n0\n0\n0\n0.108\n0.086\n0.043\n0.079\n\n\n\n\n2430 rows × 13 columns\n\n\n\n\nThe dataset contains the following columns:\n\nid: Unique identifier for each consumer.\ny1 to y4: Binary indicators showing which of the four yogurts was chosen by the consumer. Only one of these columns will have a value of 1 per row, indicating the chosen product.\nf1 to f4: Binary indicators showing whether each of the four yogurts was featured (advertised) in the store at the time of the purchase.\np1 to p4: Numeric values representing the price per ounce of each of the four yogurts.\n\nFrom the preview of the data:\n\nIn the first row, consumer with id 1 purchased yogurt 4 (as indicated by y4 = 1) at a price of 0.079 per ounce. None of the yogurts were featured.\nSubsequent rows show similar data for other consumers, indicating their choices and the conditions (price and whether the product was featured) under each product.\n\nLet the vector of product features include brand dummy variables for yogurts 1-3 (we’ll omit a dummy for product 4 to avoid multi-collinearity), a dummy variable to indicate if a yogurt was featured, and a continuous variable for the yogurts’ prices:\n\\[ x_j' = [\\mathbbm{1}(\\text{Yogurt 1}), \\mathbbm{1}(\\text{Yogurt 2}), \\mathbbm{1}(\\text{Yogurt 3}), X_f, X_p] \\]\nThe “hard part” of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariate \\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariate \\(k\\)).\nWhat we would like to do is reorganize the data from a “wide” shape with \\(n\\) rows and multiple columns for each covariate, to a “long” shape with \\(n \\times J\\) rows and a single column for each covariate. As part of this re-organization, we’ll add binary variables to indicate the first 3 products; the variables for featured and price are included in the dataset and simply need to be “pivoted” or “melted” from wide to long.\nLet’s reshape the dataset for analysis:\n\n\n\n\n\n\n\n\n\n\nid\nchoice\nfeatured\nprice\nproduct\nbrand1\nbrand2\nbrand3\n\n\n\n\n0\n1\n0\n0\n0.108\n1\n1\n0\n0\n\n\n1\n2\n0\n0\n0.108\n1\n1\n0\n0\n\n\n2\n3\n0\n0\n0.108\n1\n1\n0\n0\n\n\n3\n4\n0\n0\n0.108\n1\n1\n0\n0\n\n\n4\n5\n0\n0\n0.125\n1\n1\n0\n0\n\n\n\n\n\n\n\n\nThe dataset has been reshaped into the long format using the dataset named “yogurt”. Here is the structure of the transformed data:\n\nid: Consumer identifier.\nchoice: Indicates whether the consumer chose this product (1 if chosen, 0 otherwise).\nfeatured: Indicates whether the product was featured (1 if featured, 0 otherwise).\nprice: Price per ounce of the product.\nproduct: Product number (1 through 4).\nbrand1, brand2, brand3: Dummy variables for the first three products, with the fourth product omitted to avoid multicollinearity.\n\n\n\nEstimation\n\nUtility Function: Calculates the utility for each product based on the beta coefficients and the features in the dataset.\nProbability Calculation: Determines the probabilities of choosing each product by a consumer, factoring in the utilities of all available products for normalization.\nLog-Likelihood: Computes the log of these probabilities where the product was actually chosen (where choice == 1) and sums these values to obtain the log-likelihood for the dataset given the model parameters.\n\n\n\nLog-Likelihood Value: -3570.660714305375\n\n\nOptimizin gthe model to find the MLEs for the 5 parameters (\\(\\beta_1, \\beta_2, \\beta_3, \\beta_f, \\beta_p\\)). (Hint: you should find 2 positive and 1 negative product intercepts, a small positive coefficient estimate for featured, and a large negative coefficient estimate for price.)\n\n# Optimization to find the MLE of the beta coefficients\nresult = minimize(negative_log_likelihood, initial_beta, args=(long_data_final,), \n                  method='L-BFGS-B', bounds=bounds)\n\n# Print the optimization results\nprint(\"Optimization Success:\", result.success)\nprint(\"Estimated beta coefficients:\", result.x)\n\nOptimization Success: True\nEstimated beta coefficients: [  1.38775763   0.6435053   -3.08612024   0.48741136 -37.05807603]\n\n\n\n\nDiscussion\nWe learn…\nFrom the three product intercepts from the estimated beta coefficients for Brands 1, 2, and 3, we can gauge consumer preferences among the yogurt options relative to the baseline (Brand 4, which does not have a specific intercept and is implied as zero in the model). Here’s how to interpret the intercepts:\n\nBeta for Brand 1 (beta_1): 1.388\n\nThis positive intercept suggests that, holding other factors constant (such as price and whether the product was featured), Brand 1 is preferred over Brand 4. The higher positive value indicates a strong preference for this brand.\n\nBeta for Brand 2 (beta_2): 0.644\n\nSimilarly, this positive intercept indicates a preference for Brand 2 over Brand 4, though the preference is less strong compared to Brand 1. Brand 2 is still preferred but not as much as Brand 1.\n\nBeta for Brand 3 (beta_3): −3.086\n\nThe negative value of this intercept indicates that Brand 3 is less preferred compared to Brand 4. This substantial negative value suggests a strong aversion to Brand 3 relative to the other options.\n\n\n\n\nWhich yogurt is most preferred?\nBased on the magnitude and direction of the beta coefficients:\n\nBrand 1 is the most preferred yogurt among the options. It has the highest positive intercept, indicating the strongest preference relative to the baseline (Brand 4).\nBrand 2 is also preferred over the baseline but to a lesser extent than Brand 1.\nBrand 3 is the least preferred, with a significant negative intercept indicating a strong preference against it compared to Brand 4.\n\nThis interpretation can help in strategic decisions related to marketing and product placement, such as focusing promotional efforts on Brands 1 and 2, considering price adjustments or rebranding for Brand 3, and understanding the competitive position of Brand 4 as a potentially neutral or fallback option.\n\nIdentify the most and least preferred yogurts:\n\nMost preferred: Brand 1 (intercept = 1.388)\nLeast preferred: Brand 3 (intercept = -3.086)\n\nCalculate the difference in utilities between the most preferred and least preferred yogurts:\nUse the price coefficient as a conversion factor:\n\nPrice coefficient (beta_p): −37.058\nThe price coefficient’s magnitude indicates the decrease in the utility of purchasing a yogurt per dollar increase in its price. Therefore, it acts as a conversion factor from utility to dollar value.\n\n\n\n# Calculate the dollar benefit\ndollar_benefit = utility_difference * dollar_value_per_util\ndollar_benefit\n\n-0.12072966700847321\n\n\nThe calculated dollar benefit between the most preferred yogurt (Brand 1) and the least preferred yogurt (Brand 3) is approximately -$0.121 per unit. This indicates that consumers might be willing to pay about 12 cents more per unit for Brand 1 over Brand 3 due to their difference in utility, based on the model estimates.\nThis negative sign might initially seem counterintuitive, but it reflects how much less consumers would be willing to pay for the less preferred option (Brand 3). In other words, Brand 1 has a higher inherent value of about 12 cents per unit over Brand 3 in the eyes of consumers.\nThis monetary measure effectively quantifies the brand value in dollar terms based on consumer preferences modeled by the MNL. This insight can be leveraged for pricing strategies and marketing efforts to enhance product positioning and profitability.\nOne benefit of the MNL model is that we can simulate counterfactuals (eg, what if the price of yogurt 1 was $0.10/oz instead of $0.08/oz).\nTo calculate the market shares and predict the changes in market share due to a price increase for Yogurt 1,\n\nCalculate Current Market Shares:\n\nCount the number of times each product was chosen and divide by the total number of observations.\n\nSimulate the Effect of a Price Increase for Yogurt 1:\n\nIncrease the price of Yogurt 1 by $0.10 and use the fitted model to estimate the choice probabilities for each product and each consumer.\nCalculate the expected market shares by taking the average of the choice probabilities across all consumers for each product.\n\n\n\n# Calculate the current market shares\ncurrent_market_shares = long_data_final.groupby('product')['choice'].mean()\nprint(\"Current Market Shares:\", current_market_shares)\n\nCurrent Market Shares: product\n1    0.341975\n2    0.401235\n3    0.029218\n4    0.227572\nName: choice, dtype: float64\n\n\n\n# Call the function with the original data and fitted beta coefficients\nnew_market_shares = predict_new_market_shares(long_data_final, result.x)\nprint(\"New Market Shares After Price Increase:\", new_market_shares)\n\nNew Market Shares After Price Increase: product\n1    0.021117\n2    0.591145\n3    0.044040\n4    0.343697\nName: probability, dtype: float64\n\n\nBased on the results provided:\n\n\nCurrent Market Shares:\n\nYogurt 1: 34.2%\nYogurt 2: 40.1%\nYogurt 3: 2.9%\nYogurt 4: 22.8%\n\n\n\nNew Market Shares After a $0.10 Price Increase to Yogurt 1:\n\nYogurt 1: 2.1%\nYogurt 2: 59.1%\nYogurt 3: 4.4%\nYogurt 4: 34.4%\n\n\n\nAnalysis:\n\nYogurt 1 Market Share: The market share for Yogurt 1 drastically decreases from 34.2% to 2.1%. This significant reduction clearly indicates that the price sensitivity for Yogurt 1 is quite high, and even a modest price increase leads to a substantial shift in consumer preferences away from this product.\nShifts to Other Products: The market shares of Yogurt 2, 3, and 4 all increase. Yogurt 2 benefits the most, with its market share increasing by nearly 19 percentage points. This suggests that consumers consider Yogurt 2 a close substitute for Yogurt 1, switching to it when Yogurt 1 becomes more expensive.\nYogurt 3 and 4: Even though Yogurt 3 had a very low initial market share, it sees a relative increase, suggesting some consumers might switch to lower preference options if their first choice becomes less economically viable. Yogurt 4 also sees a notable increase in market share.\n\n\n\nConclusion:\nThe $0.10 price increase for Yogurt 1 drastically reduces its market share, confirming the price sensitivity and showing a clear preference shift among consumers to other available options, particularly Yogurt 2. This demonstrates the importance of pricing strategies in competitive markets and highlights the value of using such models for predictive analytics in marketing and pricing decisions."
  },
  {
    "objectID": "projects/Assignment3/hw3_questions.html#estimating-minivan-preferences",
    "href": "projects/Assignment3/hw3_questions.html#estimating-minivan-preferences",
    "title": "Poisson Regression Examples",
    "section": "2. Estimating Minivan Preferences",
    "text": "2. Estimating Minivan Preferences\n\nData\n\n\n\n\n\n\n\n\n\n\nresp.id\nques\nalt\ncarpool\nseat\ncargo\neng\nprice\nchoice\n\n\n\n\n0\n1\n1\n1\nyes\n6\n2ft\ngas\n35\n0\n\n\n1\n1\n1\n2\nyes\n8\n3ft\nhyb\n30\n0\n\n\n2\n1\n1\n3\nyes\n6\n3ft\ngas\n30\n1\n\n\n3\n1\n2\n1\nyes\n6\n2ft\ngas\n30\n0\n\n\n4\n1\n2\n2\nyes\n7\n3ft\ngas\n35\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n8995\n200\n14\n2\nno\n7\n3ft\ngas\n35\n1\n\n\n8996\n200\n14\n3\nno\n7\n3ft\nhyb\n35\n0\n\n\n8997\n200\n15\n1\nno\n7\n2ft\ngas\n35\n0\n\n\n8998\n200\n15\n2\nno\n8\n3ft\nelec\n40\n0\n\n\n8999\n200\n15\n3\nno\n6\n3ft\ngas\n35\n1\n\n\n\n\n9000 rows × 9 columns\n\n\n\n\nThe dataset rintro-chapter13conjoint.csv appears to contain data from a conjoint analysis, typically used to gauge consumer preferences across different product features. Here’s a breakdown of the columns:\n\nresp.id: Identifier for respondents.\nques: Question or scenario number.\nalt: Alternative options presented within each question.\ncarpool: Indicates if the option includes a carpool feature (yes/no).\nseat: Number of seats.\ncargo: Cargo space.\neng: Type of engine (e.g., ‘gas’ for gasoline, ‘hyb’ for hybrid).\nprice: Price in thousands of dollars.\nchoice: Binary indicator of whether the respondent chose this alternative (1 for chosen, 0 for not chosen).\n\n\nnum_respondents, tasks_per_respondent.describe(), alternatives_per_task\n\n(200,\n count    200.0\n mean      15.0\n std        0.0\n min       15.0\n 25%       15.0\n 50%       15.0\n 75%       15.0\n max       15.0\n Name: ques, dtype: float64,\n 3)\n\n\nHere’s a summary of the dataset based on the analysis:\n\nNumber of Respondents: There are 200 unique respondents who participated in the conjoint survey.\nChoice Tasks Per Respondent:\n\nEach respondent completed 15 choice tasks.\nThis is consistent across all respondents, as indicated by the lack of variability (standard deviation is 0).\n\nAlternatives Presented in Each Choice Task:\n\nEach choice task presented 3 alternatives to the respondents.\nThis number is consistent across all tasks and respondents, as shown by the uniform count and lack of variability.\n\n\nThe attributes (levels) were number of seats (6,7,8), cargo space (2ft, 3ft), engine type (gas, hybrid, electric), and price (in thousands of dollars).\n\n\nModel\nEstimating a MNL model omitting the following levels to avoide multicollinearity (6 seats, 2ft cargo, and gas engine). Include price as a continuous variable. Show a table of coefficients and standard errors. You may use your own likelihood function from above, or you may use a function from a package/library to perform the estimation.\n\n# Adding a constant to the model (intercept)\nX = sm.add_constant(X)\n\n# Check if there are still any object types\nprint(X.dtypes, y.dtypes)\n\nconst        float64\ncarpool        int64\nprice          int64\nseat_7         int64\nseat_8         int64\ncargo_3ft      int64\neng_hyb        int64\ndtype: object int64\n\n\n\n# Fit the MNL model\nmnl_model = sm.MNLogit(y, X)\nmnl_result = mnl_model.fit()\n\n# Print the summary of the model\nprint(mnl_result.summary())\n\nOptimization terminated successfully.\n         Current function value: 0.591150\n         Iterations 5\n                          MNLogit Regression Results                          \n==============================================================================\nDep. Variable:                 choice   No. Observations:                 9000\nModel:                        MNLogit   Df Residuals:                     8993\nMethod:                           MLE   Df Model:                            6\nDate:                Mon, 03 Jun 2024   Pseudo R-squ.:                 0.07127\nTime:                        15:43:36   Log-Likelihood:                -5320.3\nconverged:                       True   LL-Null:                       -5728.6\nCovariance Type:            nonrobust   LLR p-value:                4.060e-173\n==============================================================================\n  choice=1       coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          4.5121      0.210     21.490      0.000       4.101       4.924\ncarpool        0.0074      0.051      0.144      0.885      -0.093       0.108\nprice         -0.1486      0.006    -24.977      0.000      -0.160      -0.137\nseat_7        -0.4832      0.058     -8.397      0.000      -0.596      -0.370\nseat_8        -0.2645      0.056     -4.687      0.000      -0.375      -0.154\ncargo_3ft      0.4099      0.047      8.723      0.000       0.318       0.502\neng_hyb       -0.1102      0.050     -2.203      0.028      -0.208      -0.012\n==============================================================================\n\n\n\n\nResults\n\nPrice: The negative coefficient for price (−0.1486) indicates that as the price increases, the likelihood of choosing that option decreases, as expected.\nSeat 7 and Seat 8: Both coefficients are negative, indicating less preference compared to the baseline (6 seats). The magnitude of the coefficient for Seat 7 is larger, suggesting a stronger negative preference compared to Seat 8.\nCargo 3: The positive coefficient (0.4099) suggests a preference for more cargo space (3ft) over the baseline (2ft).\nEng Hyb: The slight negative coefficient for the hybrid engine (−0.1102) suggests a slight aversion compared to the gasoline engine, although this effect is relatively small.\n\nThese coefficients provide insights into the preferences of respondents regarding different features of the products presented in the conjoint study. The standard errors indicate the precision of these estimates, with all p-values being significant, showing strong evidence against the null hypothesis of no effect.\nUsing the price coefficient as a dollar-per-util conversion factor. What is the dollar value of 3ft of cargo space as compared to 2ft of cargo space?\n\n# Coefficients from the model\ncargo_3ft_coeff = mnl_result.params.loc['cargo_3ft'][0]\nprice_coeff = mnl_result.params.loc['price'][0]\n\n# Calculate the dollar value of 3ft of cargo space compared to 2ft\ndollar_value_cargo_3ft = cargo_3ft_coeff / -price_coeff\ndollar_value_cargo_3ft\n\n2.758956397955615\n\n\nThe dollar value of having 3ft of cargo space as compared to 2ft, based on the estimated model coefficients, is approximately $2.76.\nThis means that, according to the model, consumers would be willing to pay an additional $2.76 for a vehicle option that offers 3ft of cargo space over one that offers only 2ft, all else being equal. This calculation converts the utility gained from larger cargo space into a monetary equivalent using the sensitivity of consumer choices to changes in price.\nAssuming the market consists of the following 6 minivans. Predict the market shares of each minivan in the market.\n\n\n\nMinivan\nSeats\nCargo\nEngine\nPrice\n\n\n\n\nA\n7\n2\nHyb\n30\n\n\nB\n6\n2\nGas\n30\n\n\nC\n8\n2\nGas\n30\n\n\nD\n7\n3\nGas\n40\n\n\nE\n6\n2\nElec\n40\n\n\nF\n7\n2\nHyb\n35\n\n\n\n\n# Calculate utilities for each minivan\nutilities = np.dot(X_minivans, mnl_result.params.values)\n# Compute market shares using softmax function\nmarket_shares = np.exp(utilities) / np.sum(np.exp(utilities), axis=0)\n\n# Add market shares to the minivans dataframe\nminivans['Market Share'] = market_shares\nminivans[['Minivan', 'Market Share']]\n\n\n\n\n\n\n\n\n\nMinivan\nMarket Share\n\n\n\n\n0\nA\n0.204170\n\n\n1\nB\n0.264023\n\n\n2\nC\n0.227573\n\n\n3\nD\n0.054479\n\n\n4\nE\n0.132099\n\n\n5\nF\n0.117656\n\n\n\n\n\n\n\n\n\n\nMarket Share Interpretation\n\nMinivan B (26.40%):\n\nAttributes: 6 seats, 2ft cargo, Gas engine, priced at $30.\nInterpretation: This option has the highest market share, which suggests that a lower price combined with the base features (6 seats, Gas engine) is highly attractive to consumers. The absence of additional features seems to be offset by the lower price.\n\nMinivan C (22.76%):\n\nAttributes: 8 seats, 2ft cargo, Gas engine, also priced at $30.\nInterpretation: The second most popular option, indicating a strong preference for more seating capacity at no additional cost in price, suggesting that consumers value additional seating when all other factors are equal.\n\nMinivan A (20.42%):\n\nAttributes: 7 seats, 2ft cargo, Hybrid engine, priced at $30.\nInterpretation: The presence of a Hybrid engine with moderate pricing and seat count shows significant consumer interest in more environmentally friendly options when they are not priced higher than gas models.\n\nMinivan E (13.21%):\n\nAttributes: 6 seats, 2ft cargo, Electric engine, priced at $40.\nInterpretation: Indicates a moderate interest in electric vehicles, though the higher price seems to be a deterrent compared to cheaper gas options.\n\nMinivan F (11.77%):\n\nAttributes: 7 seats, 2ft cargo, Hybrid engine, priced at $35.\nInterpretation: Similar to Minivan A but slightly more expensive, which slightly reduces its attractiveness despite the hybrid engine.\n\nMinivan D (5.45%):\n\nAttributes: 7 seats, 3ft cargo, Gas engine, priced at $40.\nInterpretation: Despite offering more cargo space, its higher price significantly lowers its market share. This suggests that price sensitivity may outweigh the benefit of additional cargo space for many consumers.\n\n\n\n\nOverall Insights\n\nPrice Sensitivity: Price plays a crucial role in consumer choices, as seen by the higher market shares of the lower-priced models.\nPreference for Seat Capacity: There is a clear preference for higher seating capacity when the price is held constant.\nEco-friendly Options: Consumers show interest in hybrid and electric vehicles, but their choices are sensitive to price differences.\nCargo Space Value: Additional cargo space does not seem to drive higher market share, especially when accompanied by a higher price.\n\nThese insights could be used to strategize product offerings, pricing decisions, and marketing campaigns by highlighting features that align with consumer preferences.\nhint: this example is taken from the “R 4 Marketing Research” book by Chapman and Feit. I believe the same example is present in the companion book titled “Python 4 Marketing Research”. I encourage you to attempt these questions on your own, but if you get stuck or would like to compare you results to “the answers,” you may consult the Chapman and Feit books."
  },
  {
    "objectID": "projects/Assignment4/hw4_questions.html",
    "href": "projects/Assignment4/hw4_questions.html",
    "title": "Key Drivers Analysis",
    "section": "",
    "text": "This post implements a few measure of variable importance, interpreted as a key drivers analysis, for certain aspects of a payment card on customer satisfaction with that payment card.\ntodo: replicate the table on slide 19 of the session 4 slides. This involves calculating pearson correlations, standardized regression coefficients, “usefulness”, Shapley values for a linear regression, Johnson’s relative weights, and the mean decrease in the gini coefficient from a random forest. You may use packages built into R or Python.\nIf you want a challenge, either (1) implement one or more of the measures yourself. “Usefulness” is rather easy to program up. Shapley values for linear regression are a bit more work. Or (2) add additional measures to the table such as the importance scores from XGBoost.\n\n# Load the dataset\ndata_path = 'data_for_drivers_analysis.csv'\ndata = pd.read_csv(data_path)\n\n# Display the first few rows of the dataset to understand its structure\ndata.head()\n\n\n\n\n\n\n\n\n\nbrand\nid\nsatisfaction\ntrust\nbuild\ndiffers\neasy\nappealing\nrewarding\npopular\nservice\nimpact\n\n\n\n\n0\n1\n98\n3\n1\n0\n1\n1\n1\n0\n0\n1\n0\n\n\n1\n1\n179\n5\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n1\n197\n3\n1\n0\n0\n1\n1\n1\n0\n1\n1\n\n\n3\n1\n317\n1\n0\n0\n0\n0\n1\n0\n1\n1\n1\n\n\n4\n1\n356\n4\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n\n\n\n\n\n\n\n# Calculate Pearson correlation coefficients\npearson_corr = data.corr()['satisfaction'].drop('satisfaction').apply(lambda x: x * 100).round(1)\nprint(\"Pearson Correlations:\\n\", pearson_corr)\n\nPearson Correlations:\n brand        -4.9\nid           -0.9\ntrust        25.6\nbuild        19.2\ndiffers      18.5\neasy         21.3\nappealing    20.8\nrewarding    19.5\npopular      17.1\nservice      25.1\nimpact       25.5\nName: satisfaction, dtype: float64\n\n\n\n# Fit the linear regression model\nreg = LinearRegression().fit(X_std, y)\ncoefficients = reg.coef_\n\n\n# Standardized coefficients\nstd_coeff = pd.Series(coefficients, index=X.columns).apply(lambda x: x * 100).round(1)\nprint(\"Standardized Regression Coefficients:\\n\", std_coeff)\n\nStandardized Regression Coefficients:\n trust        13.6\nbuild         2.3\ndiffers       3.3\neasy          2.6\nappealing     4.0\nrewarding     0.6\npopular       1.9\nservice      10.4\nimpact       15.0\ndtype: float64\n\n\n\nusefulness_series = pd.Series(usefulness).round(1)\nprint(\"Usefulness:\\n\", usefulness_series)\n\nUsefulness:\n trust        0.8\nbuild        0.0\ndiffers      0.1\neasy         0.0\nappealing    0.1\nrewarding    0.0\npopular      0.0\nservice      0.5\nimpact       1.1\ndtype: float64\n\n\n\nshapley_values = compute_shapley_values(X, y)\nprint(\"Shapley Values:\\n\", shapley_values)\n\nShapley Values:\n trust        1.1\nbuild        0.0\ndiffers      0.1\neasy         0.0\nappealing    0.1\nrewarding    0.0\npopular      0.0\nservice      0.6\nimpact       1.5\ndtype: float64\n\n\n\n# Compute the relative weights\nrelative_weights = (eigenvectors ** 2).dot(eigenvalues) / np.sum(eigenvalues)\njohnsons_epsilon = pd.Series(relative_weights, index=X.columns).apply(lambda x: x * 100).round(1)\n\nprint(\"Johnson's Relative Weights (Epsilon):\\n\", johnsons_epsilon)\n\nJohnson's Relative Weights (Epsilon):\n trust        11.1\nbuild        11.1\ndiffers      11.1\neasy         11.1\nappealing    11.1\nrewarding    11.1\npopular      11.1\nservice      11.1\nimpact       11.1\ndtype: float64\n\n\n\n\nRandomForestRegressor()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  RandomForestRegressor?Documentation for RandomForestRegressoriFittedRandomForestRegressor() \n\n\n\n# Compute the mean decrease in Gini coefficient\ngini_importance = pd.Series(rf.feature_importances_, index=X.columns).apply(lambda x: x * 100).round(1)\n\nprint(\"Mean Decrease in Gini Coefficient:\\n\", gini_importance)\n\nMean Decrease in Gini Coefficient:\n trust        15.6\nbuild         9.8\ndiffers       9.3\neasy          9.8\nappealing     8.8\nrewarding    10.4\npopular       9.4\nservice      14.2\nimpact       12.7\ndtype: float64\n\n\n\n# Display the styled DataFrame\nstyled_results_cleaned\n\n\n\n\n\n\n\n \nPearson Correlations\nStandardized Regression Coefficients\nUsefulness\nShapley Values\nJohnson's Epsilon\nMean Decrease in Gini Coefficient\n\n\n\n\ntrust\n25.6%\n13.6%\n0.8%\n1.1%\n11.1%\n15.6%\n\n\nbuild\n19.2%\n2.3%\n0.0%\n0.0%\n11.1%\n9.8%\n\n\ndiffers\n18.5%\n3.3%\n0.1%\n0.1%\n11.1%\n9.3%\n\n\neasy\n21.3%\n2.6%\n0.0%\n0.0%\n11.1%\n9.8%\n\n\nappealing\n20.8%\n4.0%\n0.1%\n0.1%\n11.1%\n8.8%\n\n\nrewarding\n19.5%\n0.6%\n0.0%\n0.0%\n11.1%\n10.4%\n\n\npopular\n17.1%\n1.9%\n0.0%\n0.0%\n11.1%\n9.4%\n\n\nservice\n25.1%\n10.4%\n0.5%\n0.6%\n11.1%\n14.2%\n\n\nimpact\n25.5%\n15.0%\n1.1%\n1.5%\n11.1%\n12.7%"
  },
  {
    "objectID": "projects/Assignment5/hw5_questions.html",
    "href": "projects/Assignment5/hw5_questions.html",
    "title": "Segmentation Methods",
    "section": "",
    "text": "In this assignment, I implemented the K-means clustering algorithm from scratch and visualized the steps of the algorithm to gain a deeper understanding of how it works. The implementation was tested on the Iris dataset, a well-known dataset in machine learning. The objective was to compare the results of my implementation with the built-in kmeans function available in Python’s sklearn library.\nAdditionally, I calculated both the within-cluster sum-of-squares (WCSS) and silhouette scores for various numbers of clusters (K = 2, 3, …, 7). These metrics were then plotted to help determine the optimal number of clusters for the dataset. By analyzing the plots, I aimed to identify the number of clusters suggested by these two metrics.\nThis comprehensive approach not only provided practical experience with the K-means algorithm but also enhanced my understanding of evaluating clustering results using different metrics.\n\n\nFirst, I loaded the Iris dataset using the pandas library. The Iris dataset contains measurements of sepal length, sepal width, petal length, and petal width for three species of iris flowers. Loading the dataset allowed me to familiarize myself with the structure and contents of the data.\n\nimport pandas as pd\n\n# Load the Iris dataset\niris_df = pd.read_csv('iris.csv')\nprint(\"Iris Dataset:\")\nprint(iris_df.head())\n\nIris Dataset:\n   Sepal.Length  Sepal.Width  Petal.Length  Petal.Width Species\n0           5.1          3.5           1.4          0.2  setosa\n1           4.9          3.0           1.4          0.2  setosa\n2           4.7          3.2           1.3          0.2  setosa\n3           4.6          3.1           1.5          0.2  setosa\n4           5.0          3.6           1.4          0.2  setosa\n\n\n\n\n\nTo start the K-means clustering process, I needed to initialize the centroids randomly. Centroids are the center points of the clusters. I defined a function initialize_centroids that selects k random points from the dataset as the initial centroids. This randomness helps in ensuring that the centroids are spread out and can capture different parts of the data distribution.\n\n# Initialize centroids for K=3\nk = 3\ninitial_centroids = initialize_centroids(iris_data, k)\nprint(\"Initial Centroids:\")\nprint(initial_centroids)\n\nInitial Centroids:\n[[6.1 2.8 4.7 1.2]\n [5.7 3.8 1.7 0.3]\n [7.7 2.6 6.9 2.3]]\n\n\nThis step involved choosing three (since k=3) random data points to serve as the initial cluster centers. Setting a seed value ensured that the results were reproducible.\n\n\n\nNext, I needed to assign each data point to the nearest centroid. This step is crucial because it determines the initial grouping of data points. To do this, I calculated the Euclidean distance between each data point and each centroid, then assigned each point to the centroid it was closest to.\n\n# Assign clusters based on initial centroids\ninitial_clusters = assign_clusters(iris_array, initial_centroids)\nprint(\"Initial Cluster Assignments:\")\nprint(initial_clusters)\n\nInitial Cluster Assignments:\n[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2 0 0 2 0 2 0 2 0\n 0 0 0 0 0 0 2 2 0 2 0 2 0 0 2 0 0 0 2 2 2 0 0 0 2 0 0 0 0 0 0 0 2 2 0 0 0\n 0 0]\n\n\nThis assignment created an initial clustering of the data points, where each point was labeled according to the closest centroid.\n\n\n\nAfter the initial assignment of clusters, I needed to update the centroids. The new centroid for each cluster is calculated as the mean of all data points assigned to that cluster. This step helps in refining the position of the centroids to better represent the data points in their respective clusters.\n\n# Update centroids based on initial cluster assignments\nnew_centroids = update_centroids(iris_array, initial_clusters, k)\nprint(\"Updated Centroids:\")\nprint(new_centroids)\n\nUpdated Centroids:\n[[6.06904762 2.81785714 4.66309524 1.58809524]\n [5.006      3.428      1.462      0.246     ]\n [7.275      3.15625    6.18125    2.1375    ]]\n\n\nBy recalculating the centroids, the clusters become more accurate, as the centroids now reflect the mean position of all points in each cluster.\n\n\n\nTo automate the process of updating centroids and reassigning clusters, I combined the initialization, assignment, and update steps into a single function called kmeans. This function iterates until the centroids no longer change significantly, indicating convergence.\n\n# Run K-means algorithm for K=3\nk = 3\nfinal_clusters, final_centroids = kmeans(iris_array, k)\nprint(\"Final Cluster Assignments:\")\nprint(final_clusters)\nprint(\"Final Centroids:\")\nprint(final_centroids)\n\nFinal Cluster Assignments:\n[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2 2 2 2 0 2 2 2 2\n 2 2 0 0 2 2 2 2 0 2 0 2 0 2 2 0 0 2 2 2 2 2 0 2 2 2 2 0 2 2 2 0 2 2 2 0 2\n 2 0]\nFinal Centroids:\n[[5.9016129  2.7483871  4.39354839 1.43387097]\n [5.006      3.428      1.462      0.246     ]\n [6.85       3.07368421 5.74210526 2.07105263]]\n\n\nThis step ensured that the K-means algorithm could automatically refine the clusters through repeated assignments and updates, stopping once the centroids stabilized.\n\n\n\nTo evaluate the performance of the clustering, I calculated the within-cluster sum-of-squares (WCSS) for different values of K. WCSS measures the total variance within each cluster, and it’s used to determine the optimal number of clusters.\n\n# Print WCSS values\nprint(\"WCSS values:\", wcss_values)\n\nWCSS values: [152.34795176035792, 78.85144142614601, 57.38387326549491, 46.47223015873016, 39.38030324250913, 35.01796904651188]\n\n\nCalculating WCSS for multiple values of K helped me identify the “elbow point” in the plot, which suggests the optimal number of clusters.\n\n\n\nPlotting the WCSS values for different K values allowed me to visually inspect where the “elbow” point is, which indicates the optimal number of clusters.\n\nimport matplotlib.pyplot as plt\n\n# Plot WCSS values\nplt.figure(figsize=(8, 5))\nplt.plot(range(2, 8), wcss_values, marker='o')\nplt.title('Within-Cluster Sum-of-Squares (WCSS)')\nplt.xlabel('Number of Clusters (K)')\nplt.ylabel('WCSS')\nplt.show()\n\n\n\n\n\n\n\n\nThe plot showed a noticeable decrease in WCSS up to K=3, after which the rate of decrease slowed down, suggesting K=3 as the optimal number of clusters.\n\n\n\nIn addition to WCSS, I calculated silhouette scores for different K values. The silhouette score measures how similar a data point is to its own cluster compared to other clusters, providing a measure of cluster cohesion and separation.\n\nfrom sklearn.metrics import silhouette_score\n\n# Calculate silhouette scores for K=2 to K=7\nsilhouette_scores = []\nfor k in range(2, 8):\n    clusters, centroids = kmeans(iris_array, k)\n    silhouette_avg = silhouette_score(iris_array, clusters)\n    silhouette_scores.append(silhouette_avg)\n\n\n# Print silhouette scores\nprint(\"Silhouette scores:\", silhouette_scores)\n\nSilhouette scores: [0.6810461692117465, 0.5528190123564101, 0.49535632852885036, 0.4930804067193526, 0.36295529183027203, 0.34675202514223336]\n\n\nCalculating these scores for multiple K values provided another metric to evaluate the clustering performance.\n\n\n\nPlotting the silhouette scores helped me determine the optimal number of clusters by identifying the K value with the highest silhouette score.\n\n# Plot silhouette scores\nplt.figure(figsize=(8, 5))\nplt.plot(range(2, 8), silhouette_scores, marker='o')\nplt.title('Silhouette Scores')\nplt.xlabel('Number of Clusters (K)')\nplt.ylabel('Silhouette Score')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nBased on the WCSS and silhouette score plots, I concluded that K=3 was the optimal number of clusters for the Iris dataset. The WCSS plot showed a clear “elbow” at K=3, while the silhouette scores were highest for K=2 and K=3. This comprehensive analysis allowed me to successfully implement the K-means clustering algorithm and determine the optimal number of clusters for the given dataset."
  },
  {
    "objectID": "projects/Assignment5/hw5_questions.html#k-means",
    "href": "projects/Assignment5/hw5_questions.html#k-means",
    "title": "Segmentation Methods",
    "section": "",
    "text": "In this assignment, I implemented the K-means clustering algorithm from scratch and visualized the steps of the algorithm to gain a deeper understanding of how it works. The implementation was tested on the Iris dataset, a well-known dataset in machine learning. The objective was to compare the results of my implementation with the built-in kmeans function available in Python’s sklearn library.\nAdditionally, I calculated both the within-cluster sum-of-squares (WCSS) and silhouette scores for various numbers of clusters (K = 2, 3, …, 7). These metrics were then plotted to help determine the optimal number of clusters for the dataset. By analyzing the plots, I aimed to identify the number of clusters suggested by these two metrics.\nThis comprehensive approach not only provided practical experience with the K-means algorithm but also enhanced my understanding of evaluating clustering results using different metrics.\n\n\nFirst, I loaded the Iris dataset using the pandas library. The Iris dataset contains measurements of sepal length, sepal width, petal length, and petal width for three species of iris flowers. Loading the dataset allowed me to familiarize myself with the structure and contents of the data.\n\nimport pandas as pd\n\n# Load the Iris dataset\niris_df = pd.read_csv('iris.csv')\nprint(\"Iris Dataset:\")\nprint(iris_df.head())\n\nIris Dataset:\n   Sepal.Length  Sepal.Width  Petal.Length  Petal.Width Species\n0           5.1          3.5           1.4          0.2  setosa\n1           4.9          3.0           1.4          0.2  setosa\n2           4.7          3.2           1.3          0.2  setosa\n3           4.6          3.1           1.5          0.2  setosa\n4           5.0          3.6           1.4          0.2  setosa\n\n\n\n\n\nTo start the K-means clustering process, I needed to initialize the centroids randomly. Centroids are the center points of the clusters. I defined a function initialize_centroids that selects k random points from the dataset as the initial centroids. This randomness helps in ensuring that the centroids are spread out and can capture different parts of the data distribution.\n\n# Initialize centroids for K=3\nk = 3\ninitial_centroids = initialize_centroids(iris_data, k)\nprint(\"Initial Centroids:\")\nprint(initial_centroids)\n\nInitial Centroids:\n[[6.1 2.8 4.7 1.2]\n [5.7 3.8 1.7 0.3]\n [7.7 2.6 6.9 2.3]]\n\n\nThis step involved choosing three (since k=3) random data points to serve as the initial cluster centers. Setting a seed value ensured that the results were reproducible.\n\n\n\nNext, I needed to assign each data point to the nearest centroid. This step is crucial because it determines the initial grouping of data points. To do this, I calculated the Euclidean distance between each data point and each centroid, then assigned each point to the centroid it was closest to.\n\n# Assign clusters based on initial centroids\ninitial_clusters = assign_clusters(iris_array, initial_centroids)\nprint(\"Initial Cluster Assignments:\")\nprint(initial_clusters)\n\nInitial Cluster Assignments:\n[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2 0 0 2 0 2 0 2 0\n 0 0 0 0 0 0 2 2 0 2 0 2 0 0 2 0 0 0 2 2 2 0 0 0 2 0 0 0 0 0 0 0 2 2 0 0 0\n 0 0]\n\n\nThis assignment created an initial clustering of the data points, where each point was labeled according to the closest centroid.\n\n\n\nAfter the initial assignment of clusters, I needed to update the centroids. The new centroid for each cluster is calculated as the mean of all data points assigned to that cluster. This step helps in refining the position of the centroids to better represent the data points in their respective clusters.\n\n# Update centroids based on initial cluster assignments\nnew_centroids = update_centroids(iris_array, initial_clusters, k)\nprint(\"Updated Centroids:\")\nprint(new_centroids)\n\nUpdated Centroids:\n[[6.06904762 2.81785714 4.66309524 1.58809524]\n [5.006      3.428      1.462      0.246     ]\n [7.275      3.15625    6.18125    2.1375    ]]\n\n\nBy recalculating the centroids, the clusters become more accurate, as the centroids now reflect the mean position of all points in each cluster.\n\n\n\nTo automate the process of updating centroids and reassigning clusters, I combined the initialization, assignment, and update steps into a single function called kmeans. This function iterates until the centroids no longer change significantly, indicating convergence.\n\n# Run K-means algorithm for K=3\nk = 3\nfinal_clusters, final_centroids = kmeans(iris_array, k)\nprint(\"Final Cluster Assignments:\")\nprint(final_clusters)\nprint(\"Final Centroids:\")\nprint(final_centroids)\n\nFinal Cluster Assignments:\n[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2 2 2 2 0 2 2 2 2\n 2 2 0 0 2 2 2 2 0 2 0 2 0 2 2 0 0 2 2 2 2 2 0 2 2 2 2 0 2 2 2 0 2 2 2 0 2\n 2 0]\nFinal Centroids:\n[[5.9016129  2.7483871  4.39354839 1.43387097]\n [5.006      3.428      1.462      0.246     ]\n [6.85       3.07368421 5.74210526 2.07105263]]\n\n\nThis step ensured that the K-means algorithm could automatically refine the clusters through repeated assignments and updates, stopping once the centroids stabilized.\n\n\n\nTo evaluate the performance of the clustering, I calculated the within-cluster sum-of-squares (WCSS) for different values of K. WCSS measures the total variance within each cluster, and it’s used to determine the optimal number of clusters.\n\n# Print WCSS values\nprint(\"WCSS values:\", wcss_values)\n\nWCSS values: [152.34795176035792, 78.85144142614601, 57.38387326549491, 46.47223015873016, 39.38030324250913, 35.01796904651188]\n\n\nCalculating WCSS for multiple values of K helped me identify the “elbow point” in the plot, which suggests the optimal number of clusters.\n\n\n\nPlotting the WCSS values for different K values allowed me to visually inspect where the “elbow” point is, which indicates the optimal number of clusters.\n\nimport matplotlib.pyplot as plt\n\n# Plot WCSS values\nplt.figure(figsize=(8, 5))\nplt.plot(range(2, 8), wcss_values, marker='o')\nplt.title('Within-Cluster Sum-of-Squares (WCSS)')\nplt.xlabel('Number of Clusters (K)')\nplt.ylabel('WCSS')\nplt.show()\n\n\n\n\n\n\n\n\nThe plot showed a noticeable decrease in WCSS up to K=3, after which the rate of decrease slowed down, suggesting K=3 as the optimal number of clusters.\n\n\n\nIn addition to WCSS, I calculated silhouette scores for different K values. The silhouette score measures how similar a data point is to its own cluster compared to other clusters, providing a measure of cluster cohesion and separation.\n\nfrom sklearn.metrics import silhouette_score\n\n# Calculate silhouette scores for K=2 to K=7\nsilhouette_scores = []\nfor k in range(2, 8):\n    clusters, centroids = kmeans(iris_array, k)\n    silhouette_avg = silhouette_score(iris_array, clusters)\n    silhouette_scores.append(silhouette_avg)\n\n\n# Print silhouette scores\nprint(\"Silhouette scores:\", silhouette_scores)\n\nSilhouette scores: [0.6810461692117465, 0.5528190123564101, 0.49535632852885036, 0.4930804067193526, 0.36295529183027203, 0.34675202514223336]\n\n\nCalculating these scores for multiple K values provided another metric to evaluate the clustering performance.\n\n\n\nPlotting the silhouette scores helped me determine the optimal number of clusters by identifying the K value with the highest silhouette score.\n\n# Plot silhouette scores\nplt.figure(figsize=(8, 5))\nplt.plot(range(2, 8), silhouette_scores, marker='o')\nplt.title('Silhouette Scores')\nplt.xlabel('Number of Clusters (K)')\nplt.ylabel('Silhouette Score')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nBased on the WCSS and silhouette score plots, I concluded that K=3 was the optimal number of clusters for the Iris dataset. The WCSS plot showed a clear “elbow” at K=3, while the silhouette scores were highest for K=2 and K=3. This comprehensive analysis allowed me to successfully implement the K-means clustering algorithm and determine the optimal number of clusters for the given dataset."
  },
  {
    "objectID": "projects/Assignment5/hw5_questions.html#latent-class-mnl",
    "href": "projects/Assignment5/hw5_questions.html#latent-class-mnl",
    "title": "Segmentation Methods",
    "section": "Latent-Class MNL",
    "text": "Latent-Class MNL\ntodo: Use the Yogurt dataset from HW3 to estimate a latent-class MNL model. This model was formally introduced in the paper by Kamakura & Russell (1989), which you may want to read or reference. Compare the results to the standard (aggregate) MNL model from HW3. What are the differences in the parameter estimates?\ntodo: Fit the latent-class MNL model with 2, 3, …, K classes. How many classes are suggested by the BIC? The Bayesian-Schwarz Information Criterion link is a metric that assess the benefit of a better log likelihood at the expense of additional parameters to estimate – akin to the adjusted R-squared for the linear regression model. Note, however, that a lower BIC indicates a better model fit, accounting for the number of parameters in the model."
  }
]